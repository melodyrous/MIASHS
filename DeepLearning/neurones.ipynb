{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.3 64-bit ('base': conda)",
   "metadata": {
    "interpreter": {
     "hash": "f305e2f2a4f6cd5dd84caa3f417d1fe12d4f36e08711fc88f26345070ba9ecc6"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import random\n",
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neuron():\n",
    "\n",
    "    def __init__(self, input):\n",
    "\n",
    "        self.weights = rp.random.rand(n_input)\n",
    "        \n",
    "    def activation(self, x):\n",
    "\n",
    "        self.result = \n",
    "\n",
    "\n",
    "class Layer():\n",
    "\n",
    "    def __init__(self, n_input, n_neurons):\n",
    "        self.weights = np.random.rand(n_input, n_neurons)\n",
    "        self.bias = np.zeros(n_neurons)\n",
    "        self.delta = 0\n",
    "\n",
    "\n",
    "    def activation(self, x):\n",
    "        self.result = np.tanh(np.dot(x, self.weights) + self.bias)\n",
    "        return self.result\n",
    "\n",
    "    def apply_activation_derivation(self, x):\n",
    "        return 1 - x ** 2\n",
    "\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetworkInd():\n",
    "\n",
    "    def __init__(self):\n",
    "        self._layers = []\n",
    "\n",
    "    def add_layer(self, layer):\n",
    "        \"\"\"permet de rajouter une couche. Nous considérerons un ordre séquentiel allant\n",
    "de l'entrée du réseau à sa sortie\"\"\"\n",
    "        self._layers.append(layer)\n",
    "        \n",
    "    def feedforward(self, X):\n",
    "        \"\"\"activant la totalité des couches\"\"\"\n",
    "        for l in self.layers:\n",
    "            X = activate(X)\n",
    "        return X\n",
    "\n",
    "    def backpropagation(self, X, y, learning_rate):\n",
    "        output = self.feedforward(X)\n",
    "        for in reversed(range(len(self._layers))):\n",
    "            layer = self._layers[i]\n",
    "            if layer ==  self._layers[-1]:\n",
    "                layer.delta = (y - output)*layer.apply_activation_derivation(output)\n",
    "            else:\n",
    "                next_delta = self._layers[i+1].delta\n",
    "                next_w = self._layer[i+1].weights\n",
    "                layer.delta = (np.dot(next_w, next_delta))*layer.apply_activation_derivation(layer.results)\n",
    "\n",
    "            for i in range(len(self._layers)):\n",
    "                l = self._layers[i]\n",
    "                if i==0:\n",
    "                    in_ = x\n",
    "                else:\n",
    "                    in_ = self._layers[i-1].results\n",
    "                in_ = np.expand_dims(in_, axis=0)\n",
    "                layer.weights += layer.delta * in_.T * learning_rate\n",
    "                layer.bias += layer.delta * learning_rate \n",
    "\n",
    "\n",
    "    def train(self, epochs, ):\n",
    "\n",
    "\n",
    "        self.test()\n",
    "        # TODO\n",
    "\n",
    "    def test(self):\n",
    "        # TODO\n",
    "\n",
    "    \n"
   ]
  }
 ]
}